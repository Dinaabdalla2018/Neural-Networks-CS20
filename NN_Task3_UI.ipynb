{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hfFHnt1nPLti"
   },
   "source": [
    "<h1>Task 3</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtYDESXlPLtU"
   },
   "source": [
    "<h2>Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2967,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8Yw_2fiPLta"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjXTNbSqPLte"
   },
   "source": [
    "## Read csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2968,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7T1Bmo6tPLtf"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('IrisData.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2969,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid_derivative(x):\n",
    "    f = 1 / (1 + np.exp(-1*x))\n",
    "    return (f * (1 - f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2970,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Tangent activation function\n",
    "def tanh_derivative(x):\n",
    "    f = (2/(1 + np.exp(-2*x))-1)\n",
    "    return (1 - (f**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the neural network layers\n",
    "\n",
    "- Each layer consists of an input matrix, a weight matrix and an error list.\n",
    "\n",
    "    - The Input layer, is the x_train or test matrix.\n",
    "\n",
    "    - The output layer, is a (3, 1) matirx.\n",
    "    \n",
    "    - For each of the hidden layers the dimensions are (numOf neurons in this layer, numOf neurons in the previous layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2971,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network contains layers in the form of dictionaries, each layer consists of:\n",
    "# numpy array of layer weights.\n",
    "# numpy array of layer inputs.\n",
    "# net value.\n",
    "# gradient value.\n",
    "# All values are randomly intialized.\n",
    "def nn_setup(numOf_hidden_Layers, numOf_neurons):\n",
    "    # adding the number of neurons of the input layer\n",
    "    numOf_neurons.insert(0, data.shape[1])\n",
    "    # a list of dictionaries of numpy arrays, holding the layers weights, inputs, net and gradient value.\n",
    "    network = list()\n",
    "    input_layer   = {'weights':  np.random.rand(numOf_neurons[0]),\n",
    "                     'inputs': np.random.rand(numOf_neurons[0]), \n",
    "                     'net': np.random.rand(numOf_neurons[0], 1), \n",
    "                     'gradient': np.random.rand(numOf_neurons[0], 1)}\n",
    "    input_layer['weights'] = input_layer['weights'].reshape(input_layer['weights'].shape[0], 1)\n",
    "    input_layer['inputs'] = input_layer['inputs'].reshape(input_layer['inputs'].shape[0], 1)\n",
    "    \n",
    "    hidden_layers = [{'weights': np.random.rand(numOf_neurons[i], numOf_neurons[i - 1]),\n",
    "                      'inputs': np.random.rand(numOf_neurons[i], numOf_neurons[i - 1]), \n",
    "                      'net': np.random.rand(numOf_neurons[i], 1), \n",
    "                      'gradient': np.random.rand(numOf_neurons[i], 1)} for i in range(1, len(numOf_neurons))]\n",
    "    \n",
    "    output_layer  = {'weights':  np.random.rand(4, numOf_neurons[-1]),\n",
    "                     'inputs': np.random.rand(4, numOf_neurons[-1]), \n",
    "                     'net': np.random.rand(4, 1), \n",
    "                     'gradient': np.random.rand(4, 1)}\n",
    "    \n",
    "    network.append(input_layer)\n",
    "    network += hidden_layers\n",
    "    network.append(output_layer)\n",
    "    #print(network)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Backpropagation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First feed forward function\n",
    "\n",
    "- use vector/matrix multiplication to calculate net value on each layer.\n",
    "    - net  = sum(dot(layer_x, W.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2972,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network contains layers in the form of dictionaries, each layer consists of:\n",
    "# numpy array of layer weights.\n",
    "# numpy array of layer inputs.\n",
    "# net value.\n",
    "# gradient value.\n",
    "# All values are randomly intialized.\n",
    "def feed_forward1(network, input_row, activation_fn, use_bias):\n",
    "    # for each layer use vector/matrix multiplication to calculate the net value and update it in the network.\n",
    "    network[0]['inputs'] = input_row.reshape(input_row.shape[0], 1)\n",
    "    network[1]['inputs'] = input_row.reshape(input_row.shape[0], 1)\n",
    "    last_output = []\n",
    "    for i in range(1, len(network)):\n",
    "        if not use_bias:\n",
    "            network[i]['weights'][:][0] = np.zeros(network[i]['weights'].shape[1])\n",
    "        # calculate the product of the current layer's weights and inputs.\n",
    "        neurons_val = np.dot(network[i]['weights'], network[i]['inputs'])\n",
    "        \n",
    "        # apply the activation function on the cur neurons values.\n",
    "        if activation_fn is 'Sigmoid':\n",
    "            neurons_val = [sigmoid_derivative(val) for val in neurons_val]\n",
    "        else:\n",
    "            neurons_val = [tanh_derivative(val) for val in neurons_val]\n",
    "            \n",
    "        # calculate the net value.\n",
    "        network[i]['net'] = neurons_val\n",
    "        \n",
    "        # the next layer input is this layer's output\n",
    "        if i < len(network) - 1:\n",
    "            network[i + 1]['inputs'] = neurons_val\n",
    "        else:\n",
    "            last_output = [1 if max(neurons_val[1:]) is neurons_val[i] else 0 for i in range(1, len(neurons_val))]\n",
    "            last_output.insert(0, 1)\n",
    "    return network, last_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First feed backward function\n",
    "\n",
    "- use vector/matrix multiplication to calculate gradient value on each layer.\n",
    "    - Output_layer_gradient = (intended_y - predicted_y) * d_activation_fn(net)\n",
    "    - Hidden_layer_gradient_i = (gradient_(i-1) * W_i * d_activation_fn(net_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2973,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network contains layers in the form of dictionaries, each layer consists of:\n",
    "# numpy array of layer weights.\n",
    "# numpy array of layer inputs.\n",
    "# net value.\n",
    "# gradient value.\n",
    "# All values are randomly intialized.\n",
    "def feed_backward(network, intended_y, predicted_y, activation_fn):\n",
    "    # for each layer use vector/matrix multiplication to calculate the gradient value and update it in the network.\n",
    "    # calculating gradient for output layer\n",
    "    output_layer = network[len(network) - 1]\n",
    "    output_layer['gradient'] = intended_y - predicted_y\n",
    "    if activation_fn is 'Sigmoid':\n",
    "        deriv = [sigmoid_derivative(val) for val in output_layer['net']]\n",
    "    else:\n",
    "        deriv = [tanh_derivative(val) for val in output_layer['net']]\n",
    "    deriv = np.array(deriv).reshape(-1, 1)\n",
    "    net = np.array(output_layer['net']).reshape(-1, 1)\n",
    "    output_layer['gradient'].flatten()\n",
    "    np.dot(output_layer['gradient'], deriv)\n",
    "    #np.dot(output_layer['gradient'], net)\n",
    "    network[len(network) - 1] = output_layer\n",
    "    \n",
    "    # calculating gradient for hidden layers\n",
    "    previous_gradient = output_layer['gradient']\n",
    "    for i in range(len(network) - 2, 0, -1): # step=-1\n",
    "        layer = network[i]\n",
    "        next_layer = network[i + 1]\n",
    "        layer['gradient'] = np.dot(previous_gradient.T, next_layer['weights'])\n",
    "        if activation_fn is 'Sigmoid':\n",
    "            deriv = [sigmoid_derivative(val) for val in layer['net']]\n",
    "        else:\n",
    "            deriv = [tanh_derivative(val) for val in layer['net']]\n",
    "        deriv = np.array(deriv).reshape(-1, 1)\n",
    "        net = np.array(layer['net']).reshape(-1, 1)\n",
    "        layer['gradient'].flatten()\n",
    "        np.dot(layer['gradient'], deriv)\n",
    "        #np.dot(layer['gradient'], net)\n",
    "        network[i] = layer   \n",
    "        previous_gradient = layer['gradient']\n",
    "    #print('backward net', network[1]['net'])\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second feed forward function\n",
    "\n",
    "- use vector/matrix multiplication to Update the weight matrix in each layer.\n",
    "    - W_i = W_i + (learning_rate * gradient_i * x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2974,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network contains layers in the form of dictionaries, each layer consists of:\n",
    "# numpy array of layer weights.\n",
    "# numpy array of layer inputs.\n",
    "# net value.\n",
    "# gradient value.\n",
    "# All values are randomly intialized.\n",
    "def feed_forward2(network, learning_rate):\n",
    "     # for each layer use vector/matrix multiplication to calculate the new weights value and update it in the network.\n",
    "    for i in range (1 , len(network)):\n",
    "        layer = network[i]\n",
    "        net = layer['net'] * layer['gradient'] * learning_rate\n",
    "        net = net.reshape(-1, 1)\n",
    "        \n",
    "        for neuron_index in range(layer['weights'].shape[0]):\n",
    "            layer['weights'][neuron_index] += net[neuron_index]\n",
    "        network[i] = layer\n",
    "        \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation model\n",
    "\n",
    "- For each epoch call:\n",
    "\n",
    "    - Feed forward, calculating the net values for each layer.\n",
    "\n",
    "    - Feed Backward, calculating the gradient values for each layer.\n",
    "\n",
    "    - Feed forward, updating the weights for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2975,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(x_train, y_train, network, learning_rate, epochs, use_bias, activation_fn):\n",
    "    # for each epoch:\n",
    "    for i in range(epochs):\n",
    "        for ind in range(x_train.shape[0]):\n",
    "            # call feed_forward1 with the given network, row of data, activation function and use_bias.\n",
    "            network, y_prediction = feed_forward1(network, x_train[ind], activation_fn, use_bias)\n",
    "            # call feed_backward with the returned network, cur row of y_train, cur y_prediction for this row and the activation_fn\n",
    "            network = feed_backward(network, y_train[ind], np.array(y_prediction), activation_fn)\n",
    "            # call feed_forward2 with the returned network and other necessary values\n",
    "            network = feed_forward2(network, learning_rate)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2976,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x_test, y_test, network, activation_fn, use_bias):\n",
    "    y_prediction = []\n",
    "    for row in x_test:\n",
    "        network, y = feed_forward1(network, row, activation_fn, use_bias)\n",
    "        y_prediction.append(y)\n",
    "    print(y_prediction)\n",
    "    # calculating the accuracy.\n",
    "    comparison = (y_prediction == y_test)\n",
    "    co = 0\n",
    "    for l in comparison:\n",
    "        ans = True\n",
    "        for val in l:\n",
    "            ans &= val\n",
    "        if ans == True:\n",
    "            co += 1\n",
    "    accuracy = (co/y_test.shape[0]) * 100\n",
    "    return y_prediction, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2977,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwxUH6jbPLuO"
   },
   "outputs": [],
   "source": [
    "# We will be using all the 4 feauters and 3 classes.\n",
    "# The y column should be on hot encoded, meaning that if the label is c1 \n",
    "    # then it should be represented as follow, 100 and so on.\n",
    "\n",
    "def extract_data():\n",
    "    data_x = data.iloc[:, :4]\n",
    "    x0 = np.ones([150, 1]) # feature 0 for bias\n",
    "    data_x = np.append(x0, data_x, axis=1)\n",
    "    # One hot encoding the ouput column.\n",
    "    values = np.array(data['Class'])\n",
    "    # integer encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "    # binary encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    data_y = onehot_encoded\n",
    "    data_y = np.append(x0, data_y, axis=1)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.4, shuffle=True, stratify = data_y)\n",
    "    return x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2978,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pl9GMxsjPLuT"
   },
   "outputs": [],
   "source": [
    "def main(numOf_hidden_layers, numOf_nuerons, activation_fn, learning_rate, epochs, use_bias):\n",
    "    # Get the train and test data.\n",
    "    x_train, y_train, x_test, y_test = extract_data()\n",
    "    \n",
    "    # Setup the Neural Network layer.\n",
    "    network = nn_setup(numOf_hidden_layers, numOf_nuerons)\n",
    "    \n",
    "    print(network)\n",
    "    \n",
    "    # Call the backpropagation model and return the learned weights.\n",
    "    network = backpropagation(x_train, y_train, network, learning_rate, epochs, use_bias, activation_fn)\n",
    "\n",
    "    print(network)\n",
    "    \n",
    "    # Test the moddel and return its accuracy, then print it.\n",
    "    y_prediction_test, accuracy_test = test(x_test, y_test, network, activation_fn, use_bias)\n",
    "    print('Testing accuracy:\\n', accuracy_test)\n",
    "    \n",
    "    # print the confusion matrix.\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(y_test, y_prediction_test))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h1>UI</h1>\n",
    "\n",
    "- text box to take the number of desired hidden layers.\n",
    "\n",
    "- text box to take the number of desired neurons in each hidden layer.\n",
    "\n",
    "- combo box to choose the activation function, sigmoid or tanh.\n",
    "\n",
    "- text box to take the desired learning rate.\n",
    "\n",
    "- text box to take the desired number of epochs.\n",
    "\n",
    "- check box for the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2979,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2980,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "input_window = Tk()\n",
    "input_window.title(\"Neural Networks Task 3\")\n",
    "input_window.geometry(\"500x500\")\n",
    "activation_fns = [\"Sigmoid\", \"Tanh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h3>Number of Hidden Layers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2981,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of hidden Layers label\n",
    "numOf_hidden_layers_value = StringVar()\n",
    "numOf_hidden_layers_label = Label(input_window, textvariable = numOf_hidden_layers_value) \n",
    "numOf_hidden_layers_value.set(\"Number of hidden Layers\")\n",
    "numOf_hidden_layers_label.place(x=45, y=100)\n",
    "#Number of hidden Layers text\n",
    "numOf_hidden_layers_text = Entry(input_window)\n",
    "numOf_hidden_layers_text.place(x=190, y=100)\n",
    "numOf_hidden_layers_text.focus_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h3>Number of neurons in each hidden layer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2982,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of neurons hidden Layers label\n",
    "numOf_neurons_value = StringVar()\n",
    "numOf_neurons_label = Label(input_window, textvariable = numOf_neurons_value) \n",
    "numOf_neurons_value.set(\"Number of neurons in them\")\n",
    "numOf_neurons_label.place(x=30, y=140)\n",
    "#Number of neurons hidden Layers text\n",
    "numOf_neurons_text = Entry(input_window)\n",
    "numOf_neurons_text.place(x=190, y=140)\n",
    "numOf_neurons_text.focus_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h3>Activation function Dropdown List</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2983,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "#Activation fn label\n",
    "activation_fn_value = StringVar()\n",
    "activation_fn_label = Label(input_window, textvariable = activation_fn_value) \n",
    "activation_fn_value.set(\"Activaion function\")\n",
    "activation_fn_label.place(x=80, y=170)\n",
    "#Activation fn list\n",
    "activation_fn_var = StringVar(input_window)\n",
    "activation_fn = OptionMenu(input_window, activation_fn_var, *activation_fns)\n",
    "activation_fn.config(width=12, font=('Helvetica', 10))\n",
    "#activation_fn_var.set('Sigmoid') # set the default option\n",
    "activation_fn.place(x=190, y=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h3>Learning Rate</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2984,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "#learning rate label\n",
    "learning_rate_value = StringVar()\n",
    "learning_rate_label = Label(input_window, textvariable = learning_rate_value) \n",
    "learning_rate_value.set(\"Learning Rate\")\n",
    "learning_rate_label.place(x=105, y=210)\n",
    "#learning rate text\n",
    "learning_rate_text = Entry(input_window)\n",
    "learning_rate_text.place(x=193, y=210)\n",
    "learning_rate_text.focus_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h3>Epochs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2985,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "#Epochs label\n",
    "epochs_label_value = StringVar()\n",
    "epochs_label = Label(input_window, textvariable = epochs_label_value) \n",
    "epochs_label_value.set(\"Epochs\")\n",
    "epochs_label.place(x=140, y=240)\n",
    "#Epochs text\n",
    "epochs_text = Entry(input_window)\n",
    "epochs_text.place(x=193, y=240)\n",
    "epochs_text.focus_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h3>Bias</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2986,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "#Bias check box\n",
    "bias_checkbox = IntVar()\n",
    "Checkbutton(input_window, text=\"Bias\", variable=bias_checkbox).place(x=190,y=290)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h3>Training The Model Button</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2987,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "def submit_button_backpropagation():\n",
    "    layers_neurons = numOf_neurons_text.get().split()\n",
    "    layers_neurons = [int(val) for val in layers_neurons]\n",
    "    main(int(numOf_hidden_layers_text.get()), layers_neurons, activation_fn_var.get(), float(learning_rate_text.get()),\n",
    "         int(epochs_text.get()), int(bias_checkbox.get()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2988,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "#Button\n",
    "train_model_button = Button(input_window, text='Train backpropagation', width=17, command=submit_button_backpropagation)\n",
    "train_model_button.place(x=190, y=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': array([[0.18611053],\n",
      "       [0.65372292],\n",
      "       [0.66876581],\n",
      "       [0.71692895],\n",
      "       [0.02297089]]), 'inputs': array([[0.09041561],\n",
      "       [0.32516869],\n",
      "       [0.96206206],\n",
      "       [0.75836751],\n",
      "       [0.939484  ]]), 'net': array([[0.88654073],\n",
      "       [0.46473374],\n",
      "       [0.67853454],\n",
      "       [0.95480094],\n",
      "       [0.94378504]]), 'gradient': array([[0.43089787],\n",
      "       [0.56964035],\n",
      "       [0.68688641],\n",
      "       [0.04732414],\n",
      "       [0.06490416]])}, {'weights': array([[0.49544618, 0.14677507, 0.43096226, 0.17898445, 0.92416484],\n",
      "       [0.23530983, 0.64955079, 0.99948948, 0.2837376 , 0.98117071],\n",
      "       [0.33103757, 0.46584945, 0.81726112, 0.70307292, 0.2482463 ],\n",
      "       [0.73157697, 0.73279699, 0.23162023, 0.56439052, 0.20237577],\n",
      "       [0.35555124, 0.26421319, 0.08000518, 0.37553943, 0.21781555],\n",
      "       [0.08272921, 0.14182819, 0.38594965, 0.96991374, 0.49002704],\n",
      "       [0.79586288, 0.8048785 , 0.12812374, 0.39528348, 0.56840816],\n",
      "       [0.64425606, 0.14176519, 0.08286766, 0.8379847 , 0.98239912],\n",
      "       [0.92425257, 0.63895819, 0.89215762, 0.97724833, 0.94465178],\n",
      "       [0.48624758, 0.93182437, 0.55936155, 0.254639  , 0.4832596 ]]), 'inputs': array([[0.74951662, 0.87904271, 0.11816775, 0.26816112, 0.95338862],\n",
      "       [0.37232249, 0.39270029, 0.67254235, 0.26564818, 0.70234912],\n",
      "       [0.30267536, 0.07391425, 0.71644862, 0.34029045, 0.92764635],\n",
      "       [0.53468333, 0.93198824, 0.65757561, 0.54104048, 0.70292256],\n",
      "       [0.57852345, 0.57205999, 0.9918675 , 0.22452522, 0.96598995],\n",
      "       [0.57872166, 0.09518593, 0.07064457, 0.34934017, 0.40383535],\n",
      "       [0.29737268, 0.10890877, 0.26399024, 0.1603714 , 0.48582652],\n",
      "       [0.9592115 , 0.87455913, 0.3724787 , 0.0430479 , 0.1484084 ],\n",
      "       [0.0916674 , 0.2346091 , 0.85641304, 0.88791919, 0.63927174],\n",
      "       [0.5660934 , 0.09254651, 0.45309545, 0.21509607, 0.93782112]]), 'net': array([[0.79545789],\n",
      "       [0.27499994],\n",
      "       [0.26353416],\n",
      "       [0.4099998 ],\n",
      "       [0.18064887],\n",
      "       [0.83906662],\n",
      "       [0.25483939],\n",
      "       [0.5486365 ],\n",
      "       [0.22806182],\n",
      "       [0.38533877]]), 'gradient': array([[0.56626402],\n",
      "       [0.19667543],\n",
      "       [0.79885196],\n",
      "       [0.52540595],\n",
      "       [0.93231599],\n",
      "       [0.83052035],\n",
      "       [0.331742  ],\n",
      "       [0.75953334],\n",
      "       [0.06588341],\n",
      "       [0.06644022]])}, {'weights': array([[3.15056465e-01, 3.09502731e-01, 7.07469398e-01, 2.32034774e-01,\n",
      "        9.84120198e-01, 3.40968906e-01, 1.72130290e-02, 6.48551842e-01,\n",
      "        4.15949453e-01, 5.76809742e-01],\n",
      "       [1.78951167e-01, 1.21181199e-01, 5.40731358e-01, 1.52434624e-01,\n",
      "        1.85869231e-01, 7.88015934e-01, 9.34072507e-01, 5.32178477e-01,\n",
      "        8.76041420e-01, 6.45666727e-01],\n",
      "       [2.16725812e-01, 4.82338274e-01, 9.33845460e-01, 3.29986784e-01,\n",
      "        3.47512398e-01, 6.29215353e-03, 4.05097689e-01, 2.42673907e-02,\n",
      "        5.22691429e-01, 4.49544301e-01],\n",
      "       [3.45459992e-01, 5.90339325e-01, 3.29621781e-01, 3.85256698e-01,\n",
      "        5.83137638e-01, 7.80813943e-01, 7.27340494e-01, 8.44420386e-01,\n",
      "        4.05540019e-01, 5.75996284e-02],\n",
      "       [3.56013404e-01, 4.92515671e-01, 8.08838449e-01, 9.03861771e-01,\n",
      "        4.72853097e-01, 8.53047179e-01, 1.64546089e-01, 8.00116040e-01,\n",
      "        7.72584245e-01, 9.79949070e-01],\n",
      "       [3.66770193e-02, 2.03976535e-01, 4.83037059e-01, 9.14034235e-01,\n",
      "        2.99125918e-01, 3.35011099e-01, 5.97088693e-01, 4.32697462e-01,\n",
      "        2.14014546e-01, 5.06598823e-01],\n",
      "       [4.23228845e-01, 7.58350973e-01, 9.17074893e-01, 5.40767403e-01,\n",
      "        2.20491670e-01, 3.46174847e-01, 9.45282356e-01, 5.26717877e-01,\n",
      "        6.84854975e-01, 8.81235763e-02],\n",
      "       [9.59208595e-01, 6.12989729e-04, 4.54746482e-01, 4.77652226e-01,\n",
      "        8.02152627e-01, 5.44250317e-01, 6.64633969e-01, 5.33963446e-01,\n",
      "        5.30984977e-01, 4.07523203e-01],\n",
      "       [9.63312414e-01, 9.83854312e-01, 9.58249357e-01, 9.02054675e-01,\n",
      "        6.33974660e-01, 1.85344028e-01, 5.53743997e-01, 1.94091337e-01,\n",
      "        9.18778443e-01, 8.93006083e-01],\n",
      "       [2.09537498e-01, 1.26239254e-01, 8.94540132e-01, 6.79774529e-01,\n",
      "        7.42803470e-02, 3.32464423e-01, 3.98955457e-01, 3.62985124e-01,\n",
      "        2.77401210e-01, 1.69553025e-02]]), 'inputs': array([[0.86161509, 0.52181313, 0.05549356, 0.07866291, 0.74187893,\n",
      "        0.0328793 , 0.87244545, 0.77264736, 0.04362377, 0.12521993],\n",
      "       [0.35073149, 0.2488541 , 0.79907222, 0.96948845, 0.23989875,\n",
      "        0.46549118, 0.02952074, 0.08657081, 0.11837901, 0.99195756],\n",
      "       [0.26206197, 0.19938241, 0.02373787, 0.39886615, 0.78555505,\n",
      "        0.96488276, 0.31568718, 0.68030158, 0.25244858, 0.0403317 ],\n",
      "       [0.70631243, 0.23641581, 0.16439156, 0.41653843, 0.11602686,\n",
      "        0.82323985, 0.09625942, 0.67101308, 0.99703362, 0.92102015],\n",
      "       [0.71226705, 0.35727123, 0.39217671, 0.3750688 , 0.40778837,\n",
      "        0.46818147, 0.11942369, 0.94688824, 0.20419677, 0.87175485],\n",
      "       [0.63883457, 0.29716758, 0.21034185, 0.18823662, 0.06866836,\n",
      "        0.61019014, 0.93809879, 0.91762364, 0.36979665, 0.76514864],\n",
      "       [0.29029765, 0.43126434, 0.80558483, 0.47553913, 0.98801876,\n",
      "        0.56774379, 0.82754651, 0.90335396, 0.96436584, 0.43381685],\n",
      "       [0.72569747, 0.79494564, 0.58230798, 0.90324064, 0.94526718,\n",
      "        0.38141979, 0.17674778, 0.75960781, 0.38573351, 0.85326099],\n",
      "       [0.98506012, 0.66758388, 0.57118237, 0.17747958, 0.91948042,\n",
      "        0.68354552, 0.78278059, 0.49179893, 0.8633368 , 0.75578281],\n",
      "       [0.05144954, 0.29705262, 0.79695361, 0.19106492, 0.94800081,\n",
      "        0.54032409, 0.21482093, 0.44921806, 0.77782429, 0.58688023]]), 'net': array([[0.26328905],\n",
      "       [0.65101311],\n",
      "       [0.35808213],\n",
      "       [0.36837064],\n",
      "       [0.28436988],\n",
      "       [0.79488491],\n",
      "       [0.05955409],\n",
      "       [0.35159269],\n",
      "       [0.75383286],\n",
      "       [0.39995935]]), 'gradient': array([[0.58823526],\n",
      "       [0.7204268 ],\n",
      "       [0.39222227],\n",
      "       [0.69644022],\n",
      "       [0.44462257],\n",
      "       [0.2373014 ],\n",
      "       [0.67005975],\n",
      "       [0.41739203],\n",
      "       [0.78770381],\n",
      "       [0.9988991 ]])}, {'weights': array([[0.68732591, 0.37631666, 0.19765398, 0.3386299 , 0.02207155,\n",
      "        0.51207133, 0.04288046, 0.61895006, 0.29540771, 0.38897113],\n",
      "       [0.70623442, 0.76684859, 0.80412973, 0.18714673, 0.96803024,\n",
      "        0.08011681, 0.92667627, 0.31887579, 0.80941027, 0.20006216],\n",
      "       [0.06987734, 0.67685631, 0.4175055 , 0.86962473, 0.23354435,\n",
      "        0.66779359, 0.80561614, 0.49542777, 0.96600643, 0.69778265],\n",
      "       [0.66919783, 0.51929065, 0.90795873, 0.17166275, 0.76586275,\n",
      "        0.11626652, 0.61813729, 0.12120846, 0.48938015, 0.22590064]]), 'inputs': array([[0.17944988, 0.80386281, 0.58797967, 0.74535916, 0.18730085,\n",
      "        0.90237974, 0.62179063, 0.8476458 , 0.9462628 , 0.69324936],\n",
      "       [0.96484391, 0.43784664, 0.93166886, 0.79290139, 0.8455701 ,\n",
      "        0.62112812, 0.32884611, 0.64207305, 0.68751469, 0.04648423],\n",
      "       [0.24435243, 0.71131045, 0.67556738, 0.060171  , 0.38248558,\n",
      "        0.09832444, 0.94036936, 0.32824717, 0.65522688, 0.56861686],\n",
      "       [0.87668503, 0.25244746, 0.50096306, 0.61239944, 0.5696695 ,\n",
      "        0.96637852, 0.86677355, 0.1686914 , 0.44329151, 0.08734859]]), 'net': array([[0.99328648],\n",
      "       [0.10569136],\n",
      "       [0.69372496],\n",
      "       [0.04448017]]), 'gradient': array([[0.84205026],\n",
      "       [0.59528551],\n",
      "       [0.11585085],\n",
      "       [0.06409415]])}]\n",
      "[{'weights': array([[0.18611053],\n",
      "       [0.65372292],\n",
      "       [0.66876581],\n",
      "       [0.71692895],\n",
      "       [0.02297089]]), 'inputs': array([[1. ],\n",
      "       [6.3],\n",
      "       [2.3],\n",
      "       [4.4],\n",
      "       [1.3]]), 'net': array([[0.88654073],\n",
      "       [0.46473374],\n",
      "       [0.67853454],\n",
      "       [0.95480094],\n",
      "       [0.94378504]]), 'gradient': array([[0.43089787],\n",
      "       [0.56964035],\n",
      "       [0.68688641],\n",
      "       [0.04732414],\n",
      "       [0.06490416]])}, {'weights': array([[     0.        ,      0.        ,      0.        ,\n",
      "             0.        ,      0.        ],\n",
      "       [123774.14229875, 123774.5565397 , 123774.9064784 ,\n",
      "        123774.19072651, 123774.88815962],\n",
      "       [124200.70708459, 124200.84189647, 124201.19330815,\n",
      "        124201.07911994, 124200.62429332],\n",
      "       [124025.2936301 , 124025.29485012, 124024.79367336,\n",
      "        124025.12644365, 124024.76442891],\n",
      "       [123748.59239159, 123748.50105354, 123748.31684554,\n",
      "        123748.61237978, 123748.45465591],\n",
      "       [123840.96930126, 123841.02840024, 123841.2725217 ,\n",
      "        123841.8564858 , 123841.37659909],\n",
      "       [124041.59781182, 124041.60682745, 124040.93007269,\n",
      "        124041.19723243, 124041.37035711],\n",
      "       [123853.55003698, 123853.04754611, 123852.98864858,\n",
      "        123853.74376562, 123853.88818004],\n",
      "       [124018.17713777, 124017.89184339, 124018.14504282,\n",
      "        124018.23013353, 124018.19753698],\n",
      "       [123825.76003512, 123826.20561191, 123825.83314909,\n",
      "        123825.52842654, 123825.75704714]]), 'inputs': array([[1. ],\n",
      "       [6.3],\n",
      "       [2.3],\n",
      "       [4.4],\n",
      "       [1.3]]), 'net': [array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.])], 'gradient': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}, {'weights': array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
      "          0.        ,   0.        ,   0.        ,   0.        ,\n",
      "          0.        ,   0.        ],\n",
      "       [168.54784747, 168.4900775 , 168.90962766, 168.52133093,\n",
      "        168.55476554, 169.15691224, 169.30296881, 168.90107478,\n",
      "        169.24493772, 169.01456303],\n",
      "       [167.98055409, 168.24616655, 168.69767374, 168.09381506,\n",
      "        168.11134068, 167.77012043, 168.16892597, 167.78809567,\n",
      "        168.28651971, 168.21337258],\n",
      "       [161.34239344, 161.58727277, 161.32655523, 161.38219014,\n",
      "        161.58007108, 161.77774739, 161.72427394, 161.84135383,\n",
      "        161.40247346, 161.05453307],\n",
      "       [173.01121823, 173.1477205 , 173.46404328, 173.5590666 ,\n",
      "        173.12805792, 173.50825201, 172.81975092, 173.45532087,\n",
      "        173.42778907, 173.6351539 ],\n",
      "       [161.31343647, 161.48073599, 161.75979651, 162.19079369,\n",
      "        161.57588537, 161.61177055, 161.87384814, 161.70945691,\n",
      "        161.490774  , 161.78335827],\n",
      "       [169.49146582, 169.82658795, 169.98531187, 169.60900438,\n",
      "        169.28872865, 169.41441182, 170.01351933, 169.59495485,\n",
      "        169.75309195, 169.15636055],\n",
      "       [167.01983944, 166.06124383, 166.51537732, 166.53828307,\n",
      "        166.86278347, 166.60488116, 166.72526481, 166.59459429,\n",
      "        166.59161582, 166.46815405],\n",
      "       [168.12205504, 168.14259694, 168.11699199, 168.0607973 ,\n",
      "        167.79271729, 167.34408666, 167.71248663, 167.35283397,\n",
      "        168.07752107, 168.05174871],\n",
      "       [162.21440981, 162.13111156, 162.89941244, 162.68464684,\n",
      "        162.07915266, 162.33733673, 162.40382777, 162.36785743,\n",
      "        162.28227352, 162.02182761]]), 'inputs': [array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.])], 'net': [array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.])], 'gradient': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}, {'weights': array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [15.70623442, 15.76684859, 15.80412973, 15.18714673, 15.96803024,\n",
      "        15.08011681, 15.92667627, 15.31887579, 15.80941027, 15.20006216],\n",
      "       [-7.13012266, -6.52314369, -6.7824945 , -6.33037527, -6.96645565,\n",
      "        -6.53220641, -6.39438386, -6.70457223, -6.23399357, -6.50221735],\n",
      "       [-7.13080217, -7.28070935, -6.89204127, -7.62833725, -7.03413725,\n",
      "        -7.68373348, -7.18186271, -7.67879154, -7.31061985, -7.57409936]]), 'inputs': [array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.])], 'net': [array([1.]), array([9.14823772e-14]), array([2.56397381e-06]), array([2.56049169e-06])], 'gradient': array([0., 0., 0., 0.])}]\n",
      "[[1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0]]\n",
      "Testing accuracy:\n",
      " 33.33333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\tkinter\\__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-2987-75233577d4ed>\", line 5, in submit_button_backpropagation\n",
      "    int(epochs_text.get()), int(bias_checkbox.get()))\n",
      "  File \"<ipython-input-2978-18e3e383e7a4>\", line 20, in main\n",
      "    print('Confusion Matrix:\\n', confusion_matrix(y_test, y_prediction_test))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\", line 253, in confusion_matrix\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\", line 81, in _check_targets\n",
      "    \"and {1} targets\".format(type_true, type_pred))\n",
      "ValueError: Classification metrics can't handle a mix of multilabel-indicator and multiclass-multioutput targets\n"
     ]
    }
   ],
   "source": [
    "input_window.mainloop() #open window"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "name": "NN Task 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
