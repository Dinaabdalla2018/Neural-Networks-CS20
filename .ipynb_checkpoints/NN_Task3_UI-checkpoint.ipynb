{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hfFHnt1nPLti"
   },
   "source": [
    "<h1>Task 3</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtYDESXlPLtU"
   },
   "source": [
    "<h2>Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8Yw_2fiPLta"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjXTNbSqPLte"
   },
   "source": [
    "## Read csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7T1Bmo6tPLtf"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('IrisData.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-1*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Tangent activation function\n",
    "def tanh(x):\n",
    "    return (2/(1 + np.exp(-2*x))-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the neural network layers\n",
    "\n",
    "- Each layer consists of an input matrix, a weight matrix and an error list.\n",
    "\n",
    "    - The Input layer, is the x_train or test matrix.\n",
    "\n",
    "    - The output layer, is a (3, 1) matirx.\n",
    "    \n",
    "    - For each of the hidden layers the dimensions are (numOf neurons in this layer, numOf neurons in the previous layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of layer inputs.\n",
    "# list of layer weights.\n",
    "# list of net values.\n",
    "# list of gradient values.\n",
    "def nn_setup(numOf_hidden_Layers, numOf_neurons):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Backpropagation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First feed forward function\n",
    "\n",
    "- use vector/matrix multiplication to calculate net value on each layer.\n",
    "    - net  = sum(dot(layer_x, W.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward1():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First feed backward function\n",
    "\n",
    "- use vector/matrix multiplication to calculate gradient value on each layer.\n",
    "    - Output_layer_gradient = (intended_y - predicted_y) * d_activation_fn(net)\n",
    "    - Hidden_layer_gradient_i = (gradient_(i-1) * W_i * d_activation_fn(net_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_backward():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second feed forward function\n",
    "\n",
    "- use vector/matrix multiplication to Update the weight matrix in each layer.\n",
    "    - W_i = W_i + (learning_rate * gradient_i * x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward2():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation model\n",
    "\n",
    "- For each epoch call:\n",
    "\n",
    "    - Feed forward, calculating the net values for each layer.\n",
    "\n",
    "    - Feed Backward, calculating the gradient values for each layer.\n",
    "\n",
    "    - Feed forward, updating the weights for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(x_train, y_train, learning_rate, epochs, use_bias, activation_fn):\n",
    "    return ouput_layer_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x_test, y_test, w, activation_fn):\n",
    "    # Based on the activation function calculate y_predict\n",
    "    v = np.dot(w.T, x_test)\n",
    "    if activation_fn is 'Sigmoid':\n",
    "        y_prediction = [sigmoid(val) for val in v]\n",
    "    else:\n",
    "        y_prediction = [tanh(val) for val in v]\n",
    "    # manually converting the y_predict to the one hot encoding scheme.\n",
    "    y_prediction = [[1 if max(val) is val else 0 [i]for i in range(3)] for val in y_prediction]\n",
    "    # calculating the accuracy.\n",
    "    accuracy = np.mean(y_prediction == y_test.flatten()) * 100\n",
    "    return y_prediction, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwxUH6jbPLuO"
   },
   "outputs": [],
   "source": [
    "# We will be using all the 4 feauters and 3 classes.\n",
    "# The y column should be on hot encoded, meaning that if the label is c1 \n",
    "    # then it should be represented as follow, 100 and so on.\n",
    "\n",
    "def extract_data():\n",
    "    data_x = data.iloc[:, :3]\n",
    "    x0 = np.ones([150, 1]) # feature 0 for bias\n",
    "    data_x = np.append(x0, data_x, axis=1)\n",
    "    \n",
    "    # One hot encoding the ouput column.\n",
    "    values = np.array(data['Class'])\n",
    "    # integer encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "    # binary encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    data_y = onehot_encoded\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.4, shuffle=True, stratify = data_y)\n",
    "    return x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pl9GMxsjPLuT"
   },
   "outputs": [],
   "source": [
    "def main(numOf_hidden_layers, numOf_nuerons, activation_fn, learning_rate, epochs, use_bias):\n",
    "    # Get the train and test data.\n",
    "    x_train, y_train, x_test, y_test = extract_data()\n",
    "    \n",
    "    # Setup the Neural Network layer.\n",
    "    nn_setup(numOf_hidden_layers, numOf_nuerons)\n",
    "    \n",
    "    # Call the backpropagation model and return the learned weights.\n",
    "    w = backpropagation(x_train, y_train, learning_rate, epochs, use_bias, activation_fn)\n",
    "\n",
    "    # Test the moddel and return its accuracy, then print it.\n",
    "    y_prediction_test, accuracy_test = test(x_test, y_test, w, activation_fn)\n",
    "    print('Testing accuracy:\\n', accuracy_test)\n",
    "    \n",
    "    # print the confusion matrix.\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(y_test, y_prediction_test))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h1>UI</h1>\n",
    "\n",
    "- text box to take the number of desired hidden layers.\n",
    "\n",
    "- text box to take the number of desired neurons in each hidden layer.\n",
    "\n",
    "- combo box to choose the activation function, sigmoid or tanh.\n",
    "\n",
    "- text box to take the desired learning rate.\n",
    "\n",
    "- text box to take the desired number of epochs.\n",
    "\n",
    "- check box for the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "input_window = Tk()\n",
    "input_window.title(\"Neural Networks Task 3\")\n",
    "input_window.geometry(\"500x500\")\n",
    "activation_fns = [\"Sigmoid\", \"Tanh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h3>Number of Hidden Layers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of hidden Layers label\n",
    "numOf_hidden_layers_value = StringVar()\n",
    "numOf_hidden_layers_label = Label(input_window, textvariable = numOf_hidden_layers_value) \n",
    "numOf_hidden_layers_value.set(\"Number of hidden Layers\")\n",
    "numOf_hidden_layers_label.place(x=45, y=100)\n",
    "#Number of hidden Layers text\n",
    "numOf_hidden_layers_text = Entry(input_window)\n",
    "numOf_hidden_layers_text.place(x=190, y=100)\n",
    "numOf_hidden_layers_text.focus_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h3>Number of neurons in each hidden layer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of neurons hidden Layers label\n",
    "numOf_neurons_value = StringVar()\n",
    "numOf_neurons_label = Label(input_window, textvariable = numOf_neurons_value) \n",
    "numOf_neurons_value.set(\"Number of neurons in them\")\n",
    "numOf_neurons_label.place(x=30, y=140)\n",
    "#Number of neurons hidden Layers text\n",
    "numOf_neurons_text = Entry(input_window)\n",
    "numOf_neurons_text.place(x=190, y=140)\n",
    "numOf_neurons_text.focus_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h3>Activation function Dropdown List</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "#Activation fn label\n",
    "activation_fn_value = StringVar()\n",
    "activation_fn_label = Label(input_window, textvariable = activation_fn_value) \n",
    "activation_fn_value.set(\"Activaion function\")\n",
    "activation_fn_label.place(x=80, y=170)\n",
    "#Activation fn list\n",
    "activation_fn_var = StringVar(input_window)\n",
    "activation_fn = OptionMenu(input_window, activation_fn_var, *activation_fns)\n",
    "activation_fn.config(width=12, font=('Helvetica', 10))\n",
    "#activation_fn_var.set('Sigmoid') # set the default option\n",
    "activation_fn.place(x=190, y=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h3>Learning Rate</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "#learning rate label\n",
    "learning_rate_value = StringVar()\n",
    "learning_rate_label = Label(input_window, textvariable = learning_rate_value) \n",
    "learning_rate_value.set(\"Learning Rate\")\n",
    "learning_rate_label.place(x=105, y=210)\n",
    "#learning rate text\n",
    "learning_rate_text = Entry(input_window)\n",
    "learning_rate_text.place(x=193, y=210)\n",
    "learning_rate_text.focus_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h3>Epochs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "#Epochs label\n",
    "epochs_label_value = StringVar()\n",
    "epochs_label = Label(input_window, textvariable = epochs_label_value) \n",
    "epochs_label_value.set(\"Epochs\")\n",
    "epochs_label.place(x=140, y=240)\n",
    "#Epochs text\n",
    "epochs_text = Entry(input_window)\n",
    "epochs_text.place(x=193, y=240)\n",
    "epochs_text.focus_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h3>Bias</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "#Bias check box\n",
    "bias_checkbox = IntVar()\n",
    "Checkbutton(input_window, text=\"Bias\", variable=bias_checkbox).place(x=190,y=290)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "source": [
    "<h3>Training The Model Button</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "def submit_button_backpropagation():\n",
    "    layers_neurons = numOf_neurons_text.get().split()\n",
    "    layers_neurons = [int(val) for val in layers_neurons]\n",
    "    main(int(numOf_hidden_layers_text.get()), layers_neurons, activation_fn_var.get(), float(learning_rate_text.get()),\n",
    "         int(epochs_text.get()), int(bias_checkbox.get()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "#Button\n",
    "train_model_button = Button(input_window, text='Train backpropagation', width=17, command=submit_button_backpropagation)\n",
    "train_model_button.place(x=190, y=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_-ariESPLuV"
   },
   "outputs": [],
   "source": [
    "input_window.mainloop() #open window"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "name": "NN Task 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
